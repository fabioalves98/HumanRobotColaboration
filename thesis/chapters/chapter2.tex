\chapter{Collaborative Robotics}

% Introdução do capítulo?

\par In this chapter... (\textbf{dar a outline do capítulo})

\section{History}
% DOUGHT: Or historical background

% Explicar o papel do robot na industria
\par Since the 1970s, industrial robots and manipulators have been apart of production lines in many sectors of industry. They provide... (\textbf{advantages of industrial robots}). 
    % LER: Springer Handbook of Robotics - Industrial Robots, p. 1385
According to the International Federation of Robotics, in the year 2020 there were an estimated 2.7 million industrial robots in operation worldwide. [\textbf{ref}]
    % Source - World Robotics 2020 Report (Extracts)
    %        - World Robotics 2020 Report (Executive Summary)

% Explicar a necessidade de colaboração com humanos
\par This level of automation guarantees uninterrupted and efficient productivity in assembly lines, since robots do not take breaks or get tired, but at the same time they limit its flexibility and adaptability. When a manufacturing process, typically of customized products with smaller lot sizes, prioritizes flexibility and changeability over automation, these machines are useless and there is a need to involve a human worker in the process.
% 
An industrial robot is usually hard coded to do one task at a time, has limited abilities for handling complex or limp objects and cannot make decisions, therefore the close linkage of human and robot in these scenarios should result in the best of both sides. This collaborative approach can have several advantages compared to full automation, particularly when a robot can be guided by a human and simultaneously provide power assistance to him.

    % Source - Cooperation of human and machines in assembly lines, 2009

% Razão para a existência de cobots
\par Traditional industrial robots have heavy structures with fixed installation, only interact with humans during programming and are otherwise separated from them through perimeter safeguarding, stopping their motion if any obstacle breaches it. These characteristics prevent them to be used in collaboration or even coexist with human workers. Given this restrictions and needs from the industry, came the concept of Collaborative Robot (Cobot[\textbf{acro}]), firstly coined in 1996 by J. Edward Colgate and Michael Peshkin [\textbf{ref}] as '\textit{a robotic device which manipulates objects in collaboration with a human operator'}. In their work, it was a simple device with a single joint that assisted the human operator by setting up virtual surfaces which could be used to constrain and guide motion.

    % Source - COBOTS: ROBOTS FOR COLLABORATION WITH HUMAN OPERATORS, 1996

\par Year later, companies like KUKA and Universal Robots made commercially available the first industry ready cobots, denoted as industrial collaborative robots. They were light-weight, flexibly relocated and easy to teach and program, even by non-experts. Most importantly, they were equipped with power, force and speed sensors and limiters, allowing safe execution of tasks near humans, therefore allowing a shared collaborative environment. 
% 
Nowadays, cobots have evolved in such a way that they are replacing industrial robots in assembly lines, or from another perspective, industrial robots are being designed with collaboration in mind.

% TODO: Explicar a necessidade da industria de focar em Human robot collaboration
\par This technological evolution and shift has as its main reason industry needs and requirements and right now, they ask for collaboration between human and robot (\textbf{industria 5.0})
% LER: https://ec.europa.eu/info/publications/industry-50_en
%      https://www.mastercontrol.com/gxp-lifeline/3-things-you-need-to-know-about-industry-5.0/
%      https://medium.com/@marcellvollmer/what-is-industry-5-0-a363041a6f0a
%      https://www.i-scoop.eu/industry-4-0/industry-5-0/


% Isto tudo gera ênfase para a emergência da area de human robot collaboration

\section{Human-Robot Collaboration}

\par Now that the historical background is set, the scientific area in question is ready to be presented... (\textbf{melhorar intro à secção})

% Começar por explicar Human Robot Interaction
\par HRC is a subsection of the general field of study called Human-Robot Interaction (HRI[\textbf{acro}]) which according to [\textbf{ref}] is defined as '\textit{a general term for all form of interaction between humans and robots}' or '\textit{the process of conveying human intentions and interpreting task descriptions into a sequence of robot motions complying with robot capabilities and working requirements}'. It is an umbrella term used to describe a multidisciplinary field that includes knowledge and understanding from human-computer interaction, robotics, artificial intelligence, design and psychology. 
    % Source - Human–robot interaction in industrial collaborative robotics: a literature review
    %        - Human–Robot Collaboration in Manufacturing Applications: A Review
    %        - Human Centered Assistance Applications for the working environment of the future 
% FIX: There are many categorizations and views of HRI and HRC (cite many sources) but a general consensus revolving this subject...
HRI can be divided in several sub-categories based on the following four criteria:

\begin{itemize}
    \item \textbf{Workspace: }It is the overlapping space in the working range of human and robot;
    \item \textbf{Working Time: }The time the participants are working inside the shared workspace;
    \item \textbf{Aim: }The objective, focus and goal of each participant regarding the task at hand;
    \item \textbf{Contact: }Meaning intentional physical contact between the participants;
\end{itemize}

\noindent Using this four criteria, HRI can be divided in:

\begin{itemize}
    \item \textbf{Human-Robot Coexistence} (workspace and working time): Defined by the capability of simultaneously sharing the workspace between humans and robots, but operating in dissimilar tasks and not interacting with each other. They do not have a common goal and do not share contact therefore do not need to be synchronized. Robot abilities often rely only on collision avoidance.
    \item \textbf{Human-Robot Cooperation} (workspace, working time and aim): It is an upgrade over the previous category. Now humans and robots also share the same purpose in the given task. Cooperation also requires synchronization, which means that either exists a common language of communication, through instructions, gestures or voice, or machine vision is used for the robot to know when it is its time to act.
    \item \textbf{Human-Robot Collaboration} (all four criteria): The final stage of HRI, where humans and robots, who simultaneously share the same workspace, work together to perform a complex task interacting physically with one another. With force/torque sensing hardware a robot can interpret human motion and intention, and react accordingly. 
\end{itemize}

% ADD: Boa descrição de HRC
% Human–robot collaboration allows the combination of
% typical strengths of robots with some of the numerous 
% strengths of humans. Typical strengths of industrial
% robots are high stamina, high payload capacity, precision, 
% and repeatability. Strengths of human workers
% that are unmatched by any machinery comprise flexibility 
% for new production tasks, creative problem-solving
% skills, and the ability to react to unforeseen situations.

\par With HRC defined and identified inside the broader field of HRI, some of its requirements and characteristics are going to be drawn in order to proceed with a full understanding of its context.
% FIX: Dizer que os próximos aspetos são focados em cobots


\subsection{Hardware and Design}

\par The design and composition of a cobotic system can be one of the most challenging problems in this field. Industrial cobots generally operate in complex working conditions and must be able to carry motion effectively, sometimes in crowded environments, while facing unexpected events such as obstacles. This is one of the reasons that cobots are usually designed with 6 to 7 degrees of freedom (DoF[\textbf{acro}]). Another is that higher DoF provide the cobot with increased flexibility and dexterity in complex manipulation tasks.
% info proveniente do 3.1 e 3.1.1 do paper 1
% Source (+/-) - Coordination mechanism for integrated design of human-robot interaction scenarios
\par Because cobots are meant to work alongside humans, reduced weight of the moving parts is one of the main factors in cobot design. Even so, in environments where collisions are inevitable, the risk of interaction with cobots can be reduced due to their increased sensorial apparatus, such as the use of proximity-sensitive skins or force/torque sensors to detect collisions, the increased energy absorbing properties of protective layers, the limits of robot velocity and maximum strength and force, and in some scenarios the placement of airbags around the robot.
% info proveniente do 3.2.1 do paper 1
% Source (+/-) - Springer Hand Book of robotics chp. 54.5 p. 1405
%              - On a new generation of torque controlled light-weight robots, 2001
%              - Design and modeling of a compliant link for inherently safe cobots, 2017
%              - End-effector airbags to accelerate human-robot collaboration. 2017

\par According to recent reviews, research is still being develop on numerous different ways to improve the design of cobots in order to make them reliable, dependable and most importantly safer.
% Source - Human–robot interaction in industrial collaborative robotics: a literature review
%        - ultima parte do 3.2.1 do paper 1

% ADD: Falar sobre células robóticas


\subsection{Safety}

% LER: Design Considerations for Safe Human-robot Collaborative Workplace, 2015

\par The general design and composition of a cobot has been shown, but is not enough to guarantee safe HRC. According to ISO 10218:2011 "Robots and equipment for robots - Safety requirements for industrial robots", and later more widely explained in ISO/TS 15066:2016 "Robots and robotic devices — Collaborative robots", specific requirements for cobotic system need to be met for them to be considered safe. These standards define four classes of safety requirements for industrial robots in collaborative environments:

% Source FT - https://www.iso.org/standard/51330.html
%           - https://www.iso.org/standard/62996.html

% Further Details at - https://www.robotics.org/userAssets/riaUploads/file/12-TR15066Overview-SafetyforCollaborativeApplications-RobertaNelsonShea.pdf

\begin{itemize}
    \item \textbf{Safety-rated monitored stop: }The robot is stopped upon access of the human to the collaborative workspace. Most robot manufacturers offer a safety controller that assure the standstill of the robot. The robot can then resume the task once the human has left the collaborative workspace. This mode is mostly used when the cobot works alone, but occasionally a human operator can enter its workspace. In a broader view, the robot does not move while the human is present.
    \item \textbf{Hand-guiding: }The human uses a hand-operated device, usually located at the EEF of the cobot, to transmit motion commands to the robot system. This type of operation implies a direct physical interaction with the robot, where the human must have full control over the robot movement. The position of the human within the collaborative workspace must be defined and a safety controller for delimiting the robot speed is required. Graphic support through through icons or 3D simulation is helpful for intuitive programming of the robot.
    \item \textbf{Speed and separation monitoring: }There is constant monitoring of the relative speed and distance between robot and human. The robot must maintain a minimum safe distance and speed to the human in order to be able to stop any dangerous motion if contact with the human is imminent. When the separation distance decreases to a value below the minimum, the robot stops. When the human moves away from the robot, the robot can resume motion automatically. External vision sensor data might be needed to achieve this level of monitoring since few manufacturers equip cobots with such capabilities. 
    \item \textbf{Power and force limiting: }The robot system should be designed to sufficiently reduce risk to a human by allowing direct, physical interaction without an additional safety controller. This is done through the design of the robot system by limiting collision forces so that in the event of a contact between the humans and the robot biomechanical tolerance limites are not exceeded.
\end{itemize}

% Source - paper de review 2
%        - Springer handbook of robotics pg. 1407

\par A traditional industrial robot can be adapted to meet this collaborative modes and requirements, however it would need additional safety devices such as laser sensors, vision systems, or controller modifications. For this reason a commercial cobot that has this features built-in requires no further hardware costs and can be a more attractive solution.

\subsection{Programming}

% Industrial robots are hard to program
\par Industrial robots have the job of carrying out pre-programmed, repetitious tasks in order to promote productivity and efficiency. Their software platform is responsible for enabling HRI and conveying human intention on how certain tasks should be executed. Intuitive robot programming is an important issue that HRI deals with, since traditional approaches are either unintuitive of time-consuming. In the early days of industrial automation, robots could only be programmed by experts in actuators, controllers and hardware programming. Nowadays, most industrial robots are shipped with a teach pendants. These are handheld devices containing buttons, switches and in some cases a touchscreen. They are currently the most common programming method as they require little to no training in robotics or programming in general, they display the robots commands in a nontechnical fashion and allow for online editing of such commands. They also allow for walk-through programming where an operator physically moves the robot through a desired task. Although these methods seem intuitive and easy to learn, they are only applicable for certain groups of tasks since they have low accuracy requirements and for complex tasks can become cumbersome to use.
% Source - https://robotsdoneright.com/Articles/programming-methods-for-industrial-robots.html

\par Finding accurate, intuitive and efficient ways for robot programming is also a problem for HRI. One possible and currently established technique is the generation of robotic skills, which are pre-programmed software packages that only need to be parametrized by the user. A recent study [\textbf{ref}] has introduced a software architecture combining generation of robotic skills, named action blocks, that also allowed for process control, and a set of strategies and approaches for a fast and intuitive parametrization process.
% Source - Skill Parametrization Approaches and Skill Architecture for Human-Robot Interaction, 2016

\par Further research in this area is constantly looking for better user interfaces for robot programming. A few examples are:

\begin{itemize}
    \item \textbf{Virtual Reality: }Where tasks are completed intuitively as if the human is present at the remote working environment. It guarantees safe programming and is flexible on the level of workspace constraints or task complexity that may exist. The main problem is that it requires previous knowledge of the working areas to construct the virtual environment and is not suitable for loosely structured working conditions where there may be constant changes to the environment.
    \item \textbf{Augmented Reality: }Where virtual elements, mainly computer-generated graphics, are projected into the real world so that the user can perceive certain elements of robot programming in real-time. Such elements might include objects, robot motions and trajectories, and  simulated collisions with the real environment. This technique can also help operators determine the best location for the robot before final installation.
    \item \textbf{Program by Demonstration: }Where a human performs a task manually and in parallel, the robot is observing, following and learning the task in real-time. It allows any user to program a robot by just giving a demonstration of the sequence of operations to be carried out and shifts the the burden of robot programming from robot experts to task experts. It also makes possible to program more than one robot simultaneously. A significant problem with this approach is that jerks and inaccuracies in human demonstrations can lead to unsatisfactory execution of tasks by the robot that is learning them.
\end{itemize}


\par Other approaches on robot programming are also moving forward towards multimodal interfaces with the inclusion of gestures, voice, eye gaze and facial expressions serving as high-level inputs to control and program the robotic system.
% Source - 3.4 do paper 1
%        - Robot Programming using Augmented Reality, 2009


\subsection{Collaborative Tasks}


\section{Predominant Technologies}

\par In a cobotic system, the interaction between the robot, the user, the external sensors and software modules usually relies on several different technologies, all working together to provide a seamless and efficient experience. Below, divided in various categories is explained which technologies are going to be used in this work and why. 

\subsection{Robotics Middleware}

% Definição de middleware
\par A middleware is a piece of software that enables cohesive, structured communication between different software modules. It is informally described as "software glue". It has been shown that a cobotic system is comprised of different components, that usually are handled by different software modules, known as their drivers. These modules may be written in different programming languages and implement different communication protocols. As such, the job of a middleware is to hide the obvious heterogeneity resultant of a system with these characteristics, and provide functions and services that not only enhance the communication between the entities, but also serve as a flexible, interoperable and central repository of information shared by them.
% Intro ao ROS
\par The Robot Operating System (ROS[\textbf{acro}])[\textbf{ref}] is an open-source robotics middleware. With over 3.000 packages in its ecosystem, it is also described as a flexible framework for writing robot software, that comprises a plethora of tools, libraries, and conventions that aim to simplify the task of creating complex and robust robot behavior across a wide variety of robotic platforms. A key component of a cobotic system is its communication infrastructure and ROS, at its lowest level, offers a message passing interface that provides inter-process communication and implements the publisher/subscriber communication model. Each component that performs a certain task is called a ROS node, and multiple nodes communicate between them by exchanging ROS messages through ROS topics. These provide an asynchronous means of communication, since any node can publish or subscribe to any topic at any time, provided it respects its message type. For synchronous communication, ROS offers services which can be seen as remote procedure calls that provide request/response interactions between nodes. The middleware part of ROS also provides a global key-value server where nodes can set/get configuration parameters.
% Robotic features
\par When it comes to robot specific features, ROS provides libraries that helps their integration  in its ecosystem through collections of software drivers, that abstracts both the low-level control of hardware components and the treatment of information generated by sensors and other peripherals. A crucial example of such functionality is the Robot Geometry Library which helps keeping track of where different parts of the robot are with respect to each other and the world. Therefore, it allows the user to define both static transforms, such as a camera that is fixed somewhere in the world, and dynamic transforms, such as the pose of the EEF in a manipulator. This way, any positional data can be easily transformed between any pair of coordinate frames in the world.
% Extra features
\par Other features that enhance the development of such systems include RViz which provides three-dimensional visualization of robot description models, transforms and many sensor data such as point clouds; an extensive library of built-in plugins based on rqt, which is a Qt-based framework for developing graphical interfaces that easily integrate with the ROS ecosystem; a set of command-line tools that allow the user to fully control all ROS core functionality without a GUI, useful for controlling the cobotic system remotely.
% Source - https://www.ros.org/core-components/
%        - ROS: an open-source Robot Operating System

\subsection{Universal Robots UR10e}

\par Although the techniques developed in this dissertation are generic, the cobot with which this work will be implemented is an Universal Robots UR10e. It is a 6 DoF collaborative industrial robot equipped with a force/torque sensor in its EEF. Detailed technical specifications are shown in \autoref{tab:ur10e-specs} and the various ways that it can be controlled will be shown below, as well as the chosen method and why.

% Technical specifications table
\begin{table}[]
    \centering
    \begin{tabular}{|l|l|lllll}
    \cline{1-2}
    \textbf{UR10e} & \textbf{Value} &  &  &  &  &  \\ \cline{1-2}
    Reach & 1300 mm &  &  &  &  &  \\ \cline{1-2} \cline{5-7} 
    Payload & 10 Kg &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{\textbf{F/T Sensor}} & \multicolumn{1}{l|}{\textbf{Force}} & \multicolumn{1}{l|}{\textbf{Torque}} \\ \cline{1-2} \cline{5-7} 
    DoF & 6 rotating joints &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{Range} & \multicolumn{1}{l|}{100 N} & \multicolumn{1}{l|}{10 Nm} \\ \cline{1-2} \cline{5-7} 
    Pose Repeatability & +/- 0.05 mm &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{Resolution} & \multicolumn{1}{l|}{2.0 N} & \multicolumn{1}{l|}{0.02 Nm} \\ \cline{1-2} \cline{5-7} 
    Joint Working Range & ± 360 ° &  & \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{Accuracy} & \multicolumn{1}{l|}{5.5 N} & \multicolumn{1}{l|}{0.60 Nm} \\ \cline{1-2} \cline{5-7} 
    Joint Maximum Speed & ± 120 °/Sec. &  &  &  &  &  \\ \cline{1-2}
    Typical TCP Speed & 1 m/Sec. &  &  &  &  &  \\ \cline{1-2}
    Footprint & 190 mm &  &  &  &  &  \\ \cline{1-2}
    Weight & 33.5 Kg &  &  &  &  &  \\ \cline{1-2}
    \end{tabular}
    \caption{Universal Robots UR10e technical specifications}
    \label{tab:ur10e-specs}
\end{table}

\subsubsection{Teach Pendant}

\par The UR teach pendant is a handheld wired device with an 12 inch touchscreen display that allows the user to fully configure and control the UR10e. It does so using the Polyscope, a graphical user interface that operates the robot arm and control box, creates and executes programs. It terms of operational modes it allows compliant motion with the Freedrive mode, manual motion with arrow buttons and sliders, and automatic motion with the creation of programs using its built-in programming environment, which is seen as a programming tree where the user adds programming nodes. These nodes can be commands telling the robot what to do or generic programming statements (if, loop, event, etc.). Polyscope allows people with little programming experience to program the robot and for most tasks ir is done entirely using the touch panel without typing any cryptic commands. Given its simplicity, it is not ideal for complex tasks or if the user needs low-level control and information access. Besides the programming features, the device is also equipped with a button in the back that enables the Freedrive mode and an emergency stop red button in the front.

% Source - UR10e user manual 

\subsubsection{URScript}

\par Besides the Polyscope interface, UR also provides a script level way of controlling its robots through their own programming language, called URScript. This language includes variables, types, flow control statements and functions that monitor and control both I/O and robot movements. Inside the Control Box of the robot there is a low-level controller called URControl. Programming a robot at the script level is done by writing a client application and connecting to URControl using a TCP/IP socket. When a connection has been established URScript programs are sent on the socket. Compared to the Polyscope interface, URScript gives the user more control over the structure of the programs and makes it easier for an experienced user to take full advantage of the robot. One downside of this approach is the user that is sending commands externally has no access to the current state of the program, since once the program is sent, it is executed immediately without feedback. The only way the user can access the state of the robot is by connecting to another one of the available client interfaces which publish the state of the robot at a fixed rate interval.

% Source - URScript API Reference
%        - https://www.universal-robots.com/articles/ur/interface-communication/overview-of-client-interfaces/

\subsubsection{Universal Robots RTDE Interface}


\par The Real-Time Data Exchange (RTDE) interface provides a way to synchronize external application with the UR controller over a standard TCP/IP connection. This functionality is split in two stages, a setup procedure and a synchronization loop. When this loop is started, the RTDE interface sends the client the requested data in the same order it was requested by the client. On an e-Series UR cobot such as the UR10e the RTDE interface generates output messages at 500Hz. 
\par Researchers at University of Southern Denmark have developed a C++/Python for controlling and receiving data from a UR robot using the RTDE interface, called ur\_rtde. It makes available three distinct interfaces: 

\begin{itemize}
    \item \textbf{RTDE Control Interface: }Primarily used for moving the robot and utility functions. It requires a control script to be running on the robot, which is uploaded automatically.
    \item \textbf{RTDE Receive Interface: }Used for receiving data from the robot.
    \item \textbf{RTDE IO Interface: }Used for setting digital/analog IO and adjusting hte speed slider of the robot.
\end{itemize}

\noindent The Control Interface allows for non-blocking commands allowing the flow of the external program to continue while the robot is moving. The separation of Control and IO in different interfaces also allows to change the speed of the robot while it is moving. 
\par Although is takes programming knowledge and skills, from all the available ways to control a UR cobot, the use of the RTDE interface has proved to be the most advantageous one.

% Source - https://www.universal-robots.com/articles/ur/interface-communication/real-time-data-exchange-rtde-guide/
%        - https://sdurobotics.gitlab.io/ur_rtde/index.html

\subsubsection{Universal Robots ROS Driver}

\par The goal of this driver is to provide a stable and sustainable interface between UR robots and ROS, that both enhances the control of the robots using ROS paradigms such as its controller interface, and makes available to the ROS environment all data regarding the robot. By using ROS compatible manipulators, perception sensors, peripherals and motion planners the user makes sure all components speak the same language and interoperate regardless of OEM brands or communication protocols. 
\par In terms of features, this driver uses the RTDE interface for communication; uses the speed-scaling of the robot for slowing down trajectory execution accordingly; serves as a replacement for the teach pendant since it offers ROS services for most of its interactions, such as start, stop and even recover the robot from safety events; uses on-the-robot interpolation for joint-based trajectories, which helps if the application can not meet the real-time requirements of the RTDE interface; and many more extensively documented in the UR github repository.
\par This driver is currently being updated to both include with new functionality and support new UR software updates. Recent important features include joint velocity-based control that lets the user directly control the speed of each individual joint, which is very helpful for visual servoing, real-time motion planning or other kinds of control that requires speed control rather than position control, and cartesian position-based and twist-based control that let's the user execute trajectories along a cartesian path.
\par The Universal Robots ROS Driver will be the chosen method of interfacing with the UR10e for the amount of extra functionality it provides and the overall advantages of developing software in the ROS ecosystem.

% Source - https://github.com/UniversalRobots/Universal_Robots_ROS_Driver
%        - Optimizing the Universal Robots ROS driver, 2015
%        - Real-time Control of Robots with ROS, 2017


\subsection{Motion Planning}

    % LER: Springer Handbook of Robotics - Kinematics, p. 11

    \par Motion planning is a computational problem with the objective of planning motions for complex bodies from a start to a goal position. It breaks down a desired movement task into discrete motions that satisfy movement constraints while avoiding collision with known obstacles. 
    \par The MoveIt Motion Planning Framework is an easy-to-use open source robotics manipulation platform for developing commercial applications, prototyping designs, and benchmarking algorithms.

    % Source - https://picknik.ai/moveit/
    %        - David Coleman, Ioan A. Șucan, Sachin Chitta, Nikolaus Correll, Reducing the Barrier to Entry of Complex Robotic Software: a MoveIt! Case Study, Journal of Software Engineering for Robotics, 5(1):3–16, May 2014. doi: 10.6092/JOSER_2014_05_01_p3.





% Start with general introduction, then robot kinematics (fk, ik, jacobian), them global planners, them real-time motion planning
% Move It planners e afins, se calhar falar aqui dos robot kinematics



\subsection{Perception}

% % -   PCL, Euclidean Clustering, RANSAC, hand eye calibration calibration



% -   Tecnologias (software) frequentemente utilizado
% -   Falar que as plataformas de controlos dos robots são diferentes para cada robot e geralmente closed source
% -   Talvez falar sobre o UR Teach Pendant e modos de controlo do UR10e (paper)
% -   Comparar os modos de controlo (URScript, ur_rtde, ROS Driver)
% -   ROS - Senso uma das tecnologias principais, dar alguma ênfase e explicar a escolha de ROS 1 em vez de ROS 2 
% -   UR_Driver, MoveIt, ur_rtde, RViz, dynamic_reconfigure, rqt_plugins, ros_smach, plotjuggler
% -   PCL, Euclidean Clustering, RANSAC

\section{Existing approaches}

% - Pegar em 10 papers que tenham relação direta com o trabalho a desenvolver
% - Possivelmente referir teses antigas feitas no IRIS que utilizam braços robóticos
% - KUKA Sunrise Toolbox

\section{Discussion}

% - Tendo em conta o que foi dito anteriormente retirar conclusões que justifiquem o trabalho que foi feito
% -   Explicar que esta tese foca-se em real time collaborative robotics e não apenas em planeamento estático com MoveIt
% - Mais uma vez, dar ênfase nas restrições que existem em software proprietário e na falta de algoritmos e metodologias abrangentes e open source para a criação de tarefas colaborativas
% - Usar o UR teach pendant como exemplo, apontar falhas e explicar como o que se segue as pode resolver
% - Bom exemplo de como fazer um capítulo destes na tese dos drones

